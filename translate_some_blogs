A farewell to bioinformatics
March 26, 2012
I'm leaving bioinformatics to go work at a software company with more technically ept people and for a lot more money. This seems like an opportune time to set forth my accumulated wisdom and thoughts on bioinformatics.
My attitude towards the subject after all my work in it can probably be best summarized thus: "Fuck you, bioinformatics. Eat shit and die."
Bioinformatics is an attempt to make molecular biology relevant to reality. All the molecular biologists, devoid of skills beyond those of a laboratory technician, cried out for the mathematicians and programmers to magically extract science from their mountain of shitty results.
And so the programmers descended and built giant databases where huge numbers of shitty results could be searched quickly. They wrote algorithms to organize shitty results into trees and make pretty graphs of them, and the molecular biologists carefully avoided telling the programmers the actual quality of the results. When it became obvious to everyone involved that a class of results was worthless, such as microarray data, there was a rush of handwaving about "not really quantitative, but we can draw qualitative conclusions" followed by a hasty switch to a new technique that had not yet been proved worthless.
And the databases grew, and everyone annotated their data by searching the databases, then submitted in turn. No one seems to have pointed out that this makes your database a reflection of your database, not a reflection of reality. Pull out an annotation in GenBank today and it's not very long odds that it's completely wrong.
Compare this with the most important result obtained by sequencing to date: Woese et al's discovery of the archaea. (Did you think I was going to say the human genome? Fuck off. That was a monument to the vanity of that god-bobbering asshole Francis Collins, not a science project.) They didn't sequence whole genomes, or even whole genes. They sequenced a small region of the 16S rRNA, and it was chosen after pilot experiments and careful thought. The conclusions didn't require giant computers, and they didn't require precise counting of the number of templates. They knew the limitations of their tools.
Then came clinical identification, done in combination with other assays, where a judicious bit of sequencing could resolve many ambiguities. Similarly, small scale sequencing has been an incredible boon to epidemiology. Indeed, its primary scientific use is in ecology. But how many molecular biologists do you know who know anything about ecology? I can count the ones I know on one hand.
And sequencing outside of ecology? Irene Pepperberg's work with Alex the parrot dwarfs the scientific contributions of all other sequencing to date put together.
This all seems an inauspicious beginning for a field. Anything so worthless should quickly shrivel up and die, right? Well, intentionally or not, bioinformatics found a way to survive: obfuscation. By making the tools unusable, by inventing file format after file format, by seeking out the most brittle techniques and the slowest languages, by not publishing their algorithms and making their results impossible to replicate, the field managed to reduce its productivity by at least 90%, probably closer to 99%. Thus the thread of failures can be stretched out from years to decades, hidden by the cloak of incompetence.
And the rhetoric! The call for computational capacity, most of which is wasted! There are only two computationally difficult problems in bioinformatics, sequence alignment and phylogenetic tree construction. Most people would spend a few minutes thinking about what was really important before feeding data to an NP complete algorithm. I ran a full set of alignments last night using the exact algorithms, not heuristic approximations, in a virtual machine on my underpowered laptop yesterday afternoon, so we're not talking about truly hard problems. But no, the software is written to be inefficient, to use memory poorly, and the cry goes up for bigger, faster machines! When the machines are procured, even larger hunks of data are indiscriminately shoved through black box implementations of algorithms in hopes that meaning will emerge on the far side. It never does, but maybe with a bigger machine...
Fortunately for you, no one takes me seriously. The funding of molecular biology and bioinformatics is safe, protected by a wall of inbreeding, pointless jargon, and lies. So you all can rot in your computational shit heap. I'm gone.

Why do bioinformatics?
By Guillaume Filion, filed under software pollution, benchmark, bioinformatics. 

• 20 May 2015 •
I never planned to do bioinformatics. It just happened because I liked the time in front of my computer. Still, as every sane individual, I sometimes think that I could do something else with my life, and I wonder whether I am doing the right thing. On this topic, I recently came across the famous farewell to bioinformatics by Frederick J. Ross, which is worth reading, and of which the most emblematic quote is definitely the following.
My attitude towards the subject after all my work in it can probably be best summarized thus: Fuck you, bioinformatics. Eat shit and die.
There is nothing to agree or disagree in this quote, but Frederick gives further detail about his point of view in the post. In short, bioinformaticians are bad programmers, and community-level obfuscation maintains the illusion.
By making the tools unusable, by inventing file format after file format, by seeking out the most brittle techniques and the slowest languages, by not publishing their algorithms and making their results impossible to replicate, the field managed to reduce its productivity by at least 90%, probably closer to 99%.
There are indeed many issues in the bioinformatics community and I am on Frederick’s side regarding file formats. For instance, I have huge respect for the maintainers of the BAM/SAM format, but here is a quote, straight from thedocumentation*.
You do not need to know anything about C to notice that the description does not match. At some point, the core storage format of BAM has changed (just that!) and the old documentation got mixed up with the new one. So much for a planetary standard.
But no discussion of bioinformatics nonsense would be complete without a benchmark section. In our last software article, we were asked to run our benchmark against an all-pairs algorithm called slidesort. The original benchmark of slidesort concealed two minor details: that it takes months to return, and that it is not an all-pairs algorithm. The email of the maintainers being obsolete, we had to put some effort into finding the authors to ask for explanations. The answer was that it was probably a bug. But “bug” is too polite, “software pollution” is more appropriate.
... so why do bioinformatics?
The answer is simple: because it matters. Even though I deeply agree with Frederick, not everything boils down to working with skilful people. The impact of bioinformatics is unacknowledged but visible. How many discoveries started with a BLAST search? How many experiments were possible only because the human genome is sequenced? Besides, not every problem in bioinformatics is about memory footprint and CPU cycles; in some cases there are lives at stake. Choosing a treatment for cancer patients, deciding upon an abortion based on genotype data, initiating a vaccination campaign... and so much more.
Bioinformatics is biology, and it matters.
Is software a primary product of science?
When we were done writing Best Practices for Scientific Computing, we tried submitting it to a different high-profile journal than the one that ultimately accepted it (PLoS Biology, where it went on to become the most highly read article of 2014 in PLoS Biology). The response from the editor went something like this: "We recognize the importance of good engineering, but we regard writing software as equivalent to building a telescope - it's important to do it right, but we don't regard a process paper on how to build telescopes better as an intellectual contribution." (Disclaimer: I can't find the actual response, so this is a paraphrase, but it was definitely a "no" and for about that reason.)
Is scientific software like instrumentation?
When I think about scientific software as a part of science, I inevitably start with its similarities to building scientific instruments. New instrumentation and methods are absolutely essential to scientific progress, and it is clear that good engineering and methods development skills are incredibly helpful in research.
So, why did the editors at High Profile Journal bounce our paper? I infer that they drew exactly this parallel and thought no further.
But scientific software is only somewhat like new methods or instrumentation.
First, software can spread much faster and be used much more like a black box than most methods, and instrumentation inevitably involves either construction or companies that act as middlemen. With software, it's like you're shipping kits or plans for 3-D printing - something that is as close to immediately usable as it comes. If you're going to hand someone an immediately usable black box (and pitch it as such), I would argue that you should take a bit more care in building said black box.
Second, complexity in software scales much faster than in hardware (citation needed). This is partly due to human nature & a failure to think long-term, and partly due to the nature of software - software can quickly have many more moving parts than hardware, and at much less (short term) cost. Frankly, most software stacks resemble massive Rube Goldberg machines (read that link!) This means that different processes are needed here.
Third, at least in my field (biology), we are undergoing a transition to data intensive research, and software methods are becoming ever more important. There's no question that software is going to eat biology just like it's eating the rest of the world, and an increasingly large part of our primary scientific output in biology is going to hinge directly on computation (think: annotations. 'nuff said).
If we're going to build massively complex black boxes that under-pin all of our science, surely that means that theprocess is worth studying intellectually?
Is scientific software a primary intellectual output of science?
No.
I think concluding that it is is an example of the logical fallacy "affirming the consequent" - or, "confusion of necessity and sufficiency". I'm not a logician, but I would phrase it like this (better phrasing welcome!) --
Good software is necessary for good science. Good science is an intellectual contribution. Therefore good software is an intellectual contribution.
Hopefully when phrased that way it's clear that it's nonsense.
I'm naming this "the fallacy of grad student hackers", because I feel like it's a common failure mode of grad students that are good at programming. I actually think it's a tremendously dangerous idea that is confounding a lot of the discussion around software contributions in science.
To illustrate this, I'll draw the analog to experimental labs: you may have people who are tremendously good at doing certain kinds of experiments (e.g. expert cloners, or PCR wizards, or micro-injection aficionados, or WMISH bravados) and with whom you can collaborate to rapidly advance your research. They can do things that you can't, and they can do them quickly and well! But these people often face dead ends in academia and end up as eterna-postdocs, because (for better or for worse) what is valued for first authorship and career progression is intellectual contribution, and doing experiments well is not sufficient to demonstrate an intellectual contribution. Very few people get career advancement in science by simply being very good at a technique, and I believe that this is OK.
Back to software - writing software may become necessary for much of science but I don't think it should ever be sufficient as a primary contribution. Worse, it can become (often becomes?) an engine of procrastination. Admittedly, that procrastination leads to things like IPython Notebook, so I don't want to ding it, but neither are all (or even most ;) grad students like Fernando Perez, either.
Let's admit it, I'm just confused
This leaves us with a conundrum.
Software is clearly a force multiplier - "better software, better research!.
However, I don't think it can be considered a primary output of science. Dan Katz said, "Nobel prizes have been given for inventing instruments. I'm eagerly awaiting for one for inventing software [sic]" -- but I think he's wrong. Nobels have been given because of the insight enabled by inventing instruments, not for inventing instruments. (Corrections welcome!) So while I, too, eagerly await the explicit recognition that software can push scientific insight forward in biology, I am not holding my breath - I think it's going to look much more like the 2013 Chemistry Nobel, which is about general computational methodology. (My money here would be on a Nobel in Medicine for genome assembly methods, which should follow on separately from massively parallel sequencing methods and shotgun sequencing - maybe Venter, Church, and Myers/Pevzner deserve three different Nobels?)
Despite that, we do need to incentivize it, especially in biology but also more generally. Sean Eddy wrote AN AWESOME BLOG POST ON THIS TOPIC in 2010 (all caps because IT'S AWESOME AND WHY HAVEN'T WE MOVED FURTHER ON THIS <sob>). This is where DOIs for software usually come into play - hey, maybe we can make an analogy between software and papers! But I worry that this is a flawed analogy (for reasons outlined above) and will simply support the wrong idea that doing good hacking is sufficient for good science.
We also have a new problem - the so-called Big Data Brain Drain, in which it turns out that the skills that are needed for advancing science are also tremendously valuable in much more highly paid jobs -- much like physics number crunchers moving to finance, research professors in biology face a future where all our grad students go on to make more than us in tech. (Admittedly, this is only a problem if we think that more people clicking on ads is more important than basic research.) Jake Vanderplas (the author of the Big Data Brain Drain post) addressed potential solutions to this in Hacking Academia, about which I have mixed feelings. While I love both Jake and his blog post (platonically), there's a bit too much magical thinking in that post -- I don't see (m)any of those solutions getting much traction in academia.
The bottom line for me is that we need to figure it out, but I'm a bit stuck on practical suggestions. Natural selection may apply -- whoever figures this out in biology (basic research institutions and/or funding bodies) will have quite an edge in advancing biomedicine -- but natural selection works across multiple generations, and I could wish for something a bit faster. But I don't know. Maybe I'll bring it up at SciFoo this year - "Q: how can we kill off the old academic system faster?" :)
I'll leave you with two little stories.
The problem, illustrated
In 2009, we started working on what would ultimately become Pell et al., 2012. We developed a metric shit-ton of software (that's a scientific measure, folks) that included some pretty awesomely scalable sparse graph labeling approaches. The software worked OK for our problem, but was pretty brittle; I'm not sure whether or not our implementation of this partitioning approach is being used by anyone else, nor am I sure if it should be :).
However, the paper has been a pretty big hit by traditional scientific metrics! We got it into PNAS by talking about the data structure properties and linking physics, computer science, and biology together. It helped lead directly toChikhi and Rizk (2013), and it has been cited a whole bunch of times for (I think) its theoretical contributions. Yay!
Nonetheless, the incredibly important and tricky details of scalably partitioning 10 bn node graphs were lost from that paper, and the software was not a big player, either. Meanwhile, Dr. Pell left academia and moved on to a big software company where (on his first day) he was earning quite a bit more than me (good on him! I'd like a 5% tithe, though, in the future :) :). Trust me when I say that this is a net loss to academia.
Summary: good theory, useful ideas, lousy software. Traditional success. Lousy outcomes.
A contrapositive
In 2011, we figured out that linear compression ratios for sequence data simply weren't going to cut it in the face of the continued rate of data generation, and we developed digital normalization, a deceptively simple idea that hasn't really been picked up by the theoreticians. Unlike the Pell work above, it's not theoretically well studied at all. Nonetheless, the preprint has a few dozen citations (because it's so darn useful) and the work is proving to be a good foundation for further research for our lab. Perhaps the truest measure of its memetic success is that it's been reimplemented by at least three different sequencing centers.
The software is highly used, I think, and many of our efforts on the khmer software have been aimed at making diginorm and downstream concepts more robust.
Summary: lousy theory, useful ideas, good software. Nontraditional success. Awesome outcomes.
Ways forward?
I simply don't know how to chart a course forward. My current instinct (see below) is to shift our current focus much more to theory and ideas and further away from software, largely because I simply don't see how to publish or fund "boring" things like software development. (Josh Bloom has an excellent blog post that relates to this particular issue: Novelty Squared)
I've been obsessing over these topics of software and scientific focus recently (see The three porridge bowls of scientific software development and Please destroy this software after publication. kthxbye) because I'm starting to write a renewal for khmer's funding. My preliminary specific aims look something like this:
Aim 1: Expand low memory and streaming approaches for biological sequence analysis.
Aim 2: Develop graph-based approaches for analyzing genomic variation.
Aim 3: Optimize and extend a general purpose graph analysis library
Importantly, everything to do with software maintenance, support, and optimization is in Aim 3 and is in fact only a part of that aim. I'm not actually saddened by that, because I believe that software is only interesting because of the new science it enables. So I need to sell that to the NIH, and there software quality is (at best) a secondary consideration.
On the flip side, by my estimate 75% of our khmer funding is going to software maintenance, most significantly in paying down our technical debt. (In the grant I am proposing to decrease this to ~50%.)
I'm having trouble justifying this dichotomy mentally myself, and I can only imagine what the reviewers might think (although hopefully they will only glance at the budget ;).
So this highlights one conundrum: given my estimates and my priorities, how would you suggest I square these stated priorities with my funding allocations? And, in these matters, have I been wrong to focus on software quality, or should I have focused instead on accruing technical debt in the service of novel ideas and functionality? Inquiring minds want to know.
--titus
More on scientific software
Fri 24 April 2015
By C. Titus Brown
In science.
tags: sustainabilitysoftware citation
So I wrote this thing that got an awful lot of comments, many telling me that I'm just plain wrong. I think it's impossible to respond comprehensively :). But here are some responses.
What is, what could be, and what should be
In that blog post, I argued that software shouldn't be considered a primary output of scientific research. But I completely failed to articulate a distinction between what we do today with respect to scientific software, what we could be doing in the not-so-distant future, and what we should be doing. Worse, I mixed them all up!
________________________________________
Peer reviewed publications and grants are the current coin of the realm. When we submit papers and grants for peer review, we have to deal with what those reviewers think right now. In bioinformatics, this largely means papers get evaluated on their perceived novelty and impact (even in impact-blind journals). Software papers are generally evaluated poorly on these metrics, so it's hard to publish bioinformatics software papers in visible places, and it's hard to argue in grants to the NIH (and most of the biology-focused NSF) that pure software development efforts are worthwhile. This is what is, and it makes it hard for methods+software research to get publications and funding.
________________________________________
Assuming that you agree that methods+software research is important in bioinformatics, what could we be doing in the near distant future to boost the visibility of methods+software? Giving DOIs to software is one way to accrue credit to software that is highly used, but citations take a long time to pile up, reviewers won't know what to expect in terms of numbers (50 citations? is that a lot?), and my guess is that they will be poorly valued in important situations like funding and advancement. It's an honorable attempt to hack the system and software DOIs are great for other purposes, but I'm not optimistic about their near- or middle-term impact.
We could also start to articulate values and perspectives to guide reviewers and granting systems. And this is what I'd like to do. But first, let me rant a bit.
I think people underestimate the hidden mass in the scientific iceberg. Huge amounts of money are spent on research, and I would bet that there are at least twenty thousand PI-level researchers around the world in biology. In biology-related fields, any of these people may be called upon to review your grant or your paper, and their opinions will largely be, well, their own. To get published, funded, or promoted, you need to convince some committee containing these smart and opinionated researchers that what you're doing is both novel and impactful. To do that, you have to appeal largely to values and beliefs that they already hold.
Moreover, this set of researchers - largely made of people who have reached tenured professor status - sits on editorial boards, funding agency panels, and tenure and promotion committees. None of these boards and funding panels exist in a vacuum, and while to some extent program managers can push in certain directions, they are ultimately beholden to the priorities of the funding agency, which are (in the best case) channeled from senior scientists.
If you wonder why open access took so damn long to happen, this is one reason - the cultural "mass" of researchers that needs to shift their opinions is huge and unwieldy and resistant to change. And they are largely invisible, and subject to only limited persuasion.
One of the most valuable efforts we can make is to explore what we should be doing, and place it on a logical and sensical footing, and put it out there. For example, check out the CRA's memo on best practices in Promotion and Tenure of Interdisciplinary Faculty - great and thoughtful stuff, IMO. We need a bunch of well thought out opinions in this vein. What guidelines do we want to put in place for evaluating methods+software? How should we evaluate methods+software researchers for impact? When we fund software projects, what should we be looking for?
________________________________________
And that brings me to what we should be doing, which is ultimately what I am most interested in. For example, I must admit to deep confusion about what a maturity model for bioinformatics software should look like; this feeds into funding requests, which ultimately feeds into promotion and tenure. I don't know how to guide junior faculty in this area either; I have lots of opinions, but they're not well tested in the marketplace of ideas.
I and others are starting to have the opportunity to make the case for what we should be doing in review panels; what case should we make?
It is in this vein, then, that I am trying to figure out what value to place on software itself, and I'm interested in how to promote methods+software researchers and research. Neil Saunders had an interesting comment that I want to highlight here: he said,
My own feeling is that phrases like "significant intellectual contribution" are just unhelpful academic words,
I certainly agree that this is an imprecise concept, but I can guarantee that in the US, this is one of the three main questions for researchers at hiring, promotion, and tenure. (Funding opportunities and fit are my guesses for the other two.) So I would push on this point: researchers need to appear to have a clear intellectual contribution at every stage of the way, whatever that means. What it means is what I'm trying to explore.
Software is a tremendously important and critical part of the research endeavor
...but it's not enough. That's my story, and I'm sticking to it :).
I feel like the conversation got a little bit sidetracked by discussions of Nobel Prizes (mea partly culpa), and I want to discuss PhD theses instead. To get a PhD, you need to do some research; if you're a bioinformatics or biology grad student who is focused on methods+software, how much of that research can be software, and what else needs to be there?
And here again I get to dip into my own personal history.
I spent 9 years in graduate school. About 6 years into my PhD, I had a conversation with my advisor that went something like this:
________________________________________
Me, age ~27 - "Hey, Eric, I've got ~two first-author papers, and another one or two coming, along with a bunch of papers. How about I defend my PhD on the basis of that work, and stick around to finish my experimental work as a postdoc?"
Eric - blank look "All your papers are on computational methods. None of them count for your PhD."
Me - "Uhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhmmmmmmmmmm..."
(I did eventually graduate, but only after three more years of experiments.)
In biology, we have to be able to defend our computational contributions in the face of an only slowly changing professoriate. And I'm OK with that, but I think we should make it clear up front.
________________________________________
Since then, I've graduated three (soon to be five, I hope!) graduate students, one in biology and two in CS. In every single case, they've done a lot of hacking. And in every single case they've been asked to defend their intellectual contribution. This isn't just people targeting my students - I've sat on committees where students have produced masses of experimental data, and if they weren't prepared to defend their experimental design, their data interpretation, and the impact and significance of their data interpretation, they weren't read to defend. This is a standard part of the PhD process at Caltech, at MSU, and presumably at UC Davis.
So: to successfully receive a PhD, you should have to clearly articulate the problem you're tackling, its place in the scientific literature, the methods and experiments you're going to use, the data you got, the interpretation you place on that data, and the impact of their results on current scientific thinking. It's a pretty high bar, and one that I'm ok with.
________________________________________
One of the several failure modes I see for graduate students is the one where graduate students spend a huge amount of time developing software and more or less assume that this work will lead to a PhD. Why would they be thinking that?
•	Their advisor may not be particularly computational and may be giving poor guidance (which includes poorly explained criteria).
•	Their advisor may be using them (intentionally or unintentionally) - effective programmers are hard to find.
•	The grad student may be resistant to guidance.
I ticked all of these as a graduate student, but I had the advantage of being a 3rd-generation academic, so I knew the score. (And I still ran into problems.) In my previous blog post, I angered and upset some people by my blunt words (I honestly didn't think "grad student hacker fallacy" was so rude ;( but it's a real problem that I confront regularly.
Computational PhD students need to do what every scientific PhD student needs to do: clearly articulate their problem, place it in the scientific literature, define the computational methods and experiments they're going to do/have done, explain the data and their interpretation of it, and explore how it impacts science. Most of this involves things other than programming and running software! It's impossible to put down percent effort estimates that apply broadly, but my guess is that PhD students should spend at least a year understanding your results and interpreting and explaining their work.
Conveniently, however, once you've done that for your PhD, you're ready to go in the academic world! These same criteria (expanded in scope) apply to getting a postdoc, publishing as a postdoc, getting a faculty position, applying for grants, and getting tenure. Moreover, I believe many of the same criteria apply broadly to research outside of academia (which is one reason I'm still strongly +1 on getting a PhD, no matter your ultimate goals).
(Kyle Cranmer's comment on grad student efforts here was perfect.)
Software as...
As far as software being a primary product of research -- Konrad Hinsen nails it. It's not, but neither are papers, and I'm good with both statements :). Read his blog post for the full argument. The important bit is that very little stands on its own; there always needs to be communication effort around software, data, and methods.
Ultimately, I learned a lot by admitting confusion! Dan Katz and Konrad Hinsen pointed out that software is communication, and Kai Blin drew a great analogy between software and experimental design. These are perspectives that I hadn't seen said so clearly before and they've really made me think differently; both are interesting and provocative analogies and I'm hoping that we can develop them further as a community.
How do we change things?
Kyle Cranmer and Rory Kirchner had a great comment chain on broken value systems and changing the system. I love the discussion, but I'm struggling with how to respond. My tentative and mildly unhappy conclusion is that I may have bought into the intellectual elitism of academia a bit too much (see: third generation academic), but this may also be how I've gotten where I am, so... mixed bag? (Rory made me feel old and dull, too, which is pretty cool in a masochistic kind of way.)
One observation is that, in software, novelty is cheap. It's very, very easy to tweak something minorly, and fairly easy to publish it without generating any new understanding. How do we distinguish a future Heng Li or an Aaron Quinlan (who have enabled new science by cleanly solving "whole classes of common problems that you don't even have to think about anymore") from humdrum increment, and reward them properly in the earlier stages of their career? I don't know, but the answer has to be tied to advancing science, which is hard to measure on any short timescale. (Sean Eddy's blog post has the clearest view on solutions that I've yet seen.)
Another observation (nicely articulated by Daisie Huang) is that (like open data) this is another game theoretic situation, where the authors of widely used software sink their time and energy into the community but don't necessarily gain wide recognition for their efforts. There's a fat middle ground of software that's reasonably well used but isn't samtools, and this ecosystem needs to be supported. This is much harder to argue - it's a larger body of software, it's less visible, and it's frankly much more expensive to support. (Carl Boettiger's comment is worth reading here.) The funding support isn't there, although that might change in the next decade. (This is the proximal challenge for me, since I place my own software, khmer, in this "fat middle ground"; how do I make a clear argument for funding?)
Kyle Cranmer and others pointed to some success in "major instrumentation" and methods-based funding and career paths in physics (help, can't find link/tweets!). This is great, but I think it's also worth discussing the overall scale of things. Physics has a few really big and expensive instruments, with a few big questions, and with thousands of engineers devoted to them. Just in sequencing, biology has thousands (soon millions) of rather cheap instruments, devoted to many thousands of questions. If my prediction that software will "eat" this part of the world becomes true, we will need tens of thousands of data intensive biologists at a minimum, most working to some large extent on data analysis and software. I think the scale of the need here is simply much, much larger than in physics.
I am supremely skeptical of the idea that universities as we currently conceive of them are the right home for stable, mature software development. We either need to change universities in the right way (super hard) or find other institutions (maybe easier). Here, the model to watch may well be the Center for Open Science, which produces theOpen Science Framework (among others). My interpretation is that they are trying to merge scientific needs with the open source development model. (Tellingly, they are doing so largely with foundation funding; the federal funding agencies don't have good mechanisms for funding this kind of thing in biology, at least.) This may be the right model (or at least on the path towards one) for sustained software development in the biological sciences: have an institution focused on sustainability and quality, with a small diversity of missions, that can afford to spend the money to keep a number of good software engineers focused on those missions.
________________________________________
Thanks, all, for the comments and discussions!
--titus
You write: "... novelty is cheap. It's very, very easy to tweak something minorly, and fairly easy to publish it without generating any new understanding."
I suspect that of most researchers saw just that part of the quote out of context; they would readily agree with it but assume you were referring to academic publication in general rather than software in particular. I certainly would. I think you highlight issues that are both distinct to software and issues that are more general.
As you've stated here, software isn't an intellectual contribution, but a means of communicating an intellectual contribution. Some people can use the software to do new research without understanding everything that went into it; just as some people will cite the results of a paper without replicating it from scratch. Others might dig into the raw code just as others dig into raw data; perhaps finding bugs or new directions.
Some software is the result of a deep and direct intellectual discovery; a communication of a new algorithm or new theorem. Other software is instead primarily the communication of an pre-existing idea, communicated in a way that lets more people take advantage of that contribution. Yet this also happens in publications having nothing to do with software -- papers that communicate an idea that first appeared in some other field: a statistical inference method, or a way of thinking. These translational papers are frequently valuable, and can become widely cited and recognized.
Any communication can be done well or poorly, and software is no exception. To me, the good software practices you have often advocated for: unit testing, modularity, documentation; play the same role as writing up a paper with carefully crafted language and figures. Do it well, and more people will be able to use your idea. Do it terribly, and even a great idea won't go far. Do it really well, and even a simple idea can go really far.
Does that mean universities are the home for 'stable, mature software development?' Researchers rely every day on software developed at universities; just as we rely on research published at universities. That doesn't make it analogous to industry standards, any more than a scientific paper is immediately put into medical practice. Yet academics will continue to have ideas that can be communicated best in the medium of software (inferences, algorithms analyses), and doing so with a minimum of typos and jargon is as important here as in writing.
Is academic software that follows best practices really more widely used than software that doesn't? I have only anecdotes; but then the same is true for good writing style. There are some brilliant but impenetrable papers out there that still have huge impact (usually after some writes a wrapper paper for the masses), just as there are widely used but terribly engineered software. There's even some bad ideas that are widely used because some software or some paper has made them broadly accessible.
My point is that novelty, intellectual contributions, and impact of doing science vs writing (or coding) about it isn't unique to software. The battle to balance new ideas against communicating, testing, or translating existing ideas has always been part of science and we always need both, hence both doing research and publishing research have long been emphasized. Likewise, the debate on the value of technical skills spans all research and isn't isolated to undervaluing of research software engineers, and a PhD candidate should be able to articulate their intellectual contributions regardless of how much coding or pipetting they did.
The real difference between software and papers is not in the intellectual contributions they communicate, or in the impact of that communication, but in the proxies we use to measure them. Generally speaking, publications are peer reviewed & software is not. Publications appear in stratified journals with a widely recognized social ranking, software does not. Without even waiting for citations to appear (itself a crude metric; intended for provenance and co-opted for status) we have a widely recognized way to measure those contributions, and make the case for funding activities that lead to those results. In the face of these proxies for quality, we don't always ask about what the intellectual contribution was, and what the ramifications have been. If software contributions competed for spots to be highlighted in the top journals while research papers appeared naked of these proxies like a big preprint archive, would this discussion be different?
I don't know what the right model is or how it will work out. The big questions about supporting long term technical expertise, whether it's a research software engineer or the engineers "operating a 100-ton, 27-km superfluid helium system at 1.9 kelvin" are deep and difficult issues germane to doing big and hard science, and I think we need to tackle these issues as a community of researchers, not just as the sub-community of those who use software. I think our software community can do more to increase the recognition that software is having impact in academic research that is often equal to or beyond that of the best recognized papers. And I think in order for research software to win that reputation, it needs a bit of a similar spit and polish that we lavish on those papers. Perhaps that means we need proxies of quality, but it also means we need well written software.
Software in scientific research
In a recent blog post, Titus Brown asks if software is a primary product of science, and basically says “no” (but do read the post for the details). A blog-post length reply by Daniel Katz comes to the opposite conclusion (again, please read the post before continuing here). I left a short comment on Titus’ blog but also felt compelled to expand this into a blog post of its own – so here it is.
Titus introduces a useful criterion for what “primary product of science” is: could you get a Nobel prize for it? As Dan comments, Nobel prizes in science are awarded for discoveries and inventions. There we no computers when Alfred Nobel set up his foundation, so we have to extrapolate this definition a bit to today’s situation. Is software like a discovery? Clearly not. Like an invention? Perhaps, but it doesn’t fit very well. Dan makes a comparison with scientific writing, i.e. papers, textbooks, etc. Scientific writing is the traditional way to communicate discoveries and inventions. But what scientists get Nobel prizes for is not the papers, but the work described therein. Papers are not primary products of science either, they are just a means of communication. There is a fairly good analogy between papers and their contents on one hand, and software and algorithms on the other hand. And algorithms are very well comparable to discoveries and inventions. Moreover, many of today’s scientific models are in fact expressed as algorithms. My conclusion is that algorithms clearly count as a primary product of science, but software doesn’t. Software is a means of communication, just like papers or textbooks.
The analogy isn’t perfect, however. The big difference between a paper and a piece of software is that you can feed the latter into a computer to make it dosomething. Software is thus a scientific tool a well as a means of communication. In fact, today’s computational science gives more importance to the tool aspect than to the communication aspect. The main questions asked about scientific software are “What does it do?” and “How efficient is it?” When considering software as a means of communication, we would ask questions such as “Is it well-written, clear, elegant?”, “How general is the formulation?”, or “Can I use it as the basis for developing new science?”. These questions are beginning to be heard, in the context of the scientific software crisis and the need for reproducible research. But they are still second thoughts. We actually accept as normal that the scientific contents of software, i.e. the models implemented by it, are understandable only to software specialists, meaning that for the majority of users, the software is just a black box. Could you imagine this for a paper? “This paper is very obscure, but the people who wrote it are very smart, so let’s trust them and base our research on their conclusions.” Did you ever hear such a claim? Not me.
Scientists haven’t yet fully grasped the particular status of software as both an information carrier and a tool. That may be one of the few characteristics they share with lawyers. The latter make a difference between “data” (including written text), which is covered by copyright, and “software”, which is covered by both copyright and licenses, and in some countries also by patents. Superficially, this makes sense, as it reflects the dual nature of software. It suffers, however, from two problems. First of all, the distinction exists only in the intention of the author, which is hard to pin down. Software is just data that can be interpreted as instructions for a computer. One could conceivably write some interpreter that turns previously generated data into software by executing it. Second, and that’s a problem for science, the licensing aspect of software is much more restrictive than the copyright aspect. If you describe an algorithm informally in a paper, you have to deal only with copyright. If you communicate it in executable form, you have to worry about licensing and patents as well, even if your main intention is more precise communication.
I have written a detailed article about the problems resulting from the badly understood dual nature of scientific software, which I won’t repeat here. I have also proposed a solution, the development of formal languages for expressing complex scientific models, and I am experimenting with a concrete approach to get there. I mention this here mainly to motivate my conclusion:
•	Q: Is software a primary product of science?
•	A: No. But neither is a paper or a textbook.
•	Q: Is software a means of communication for primary products of science?
•	A: Yes, but it’s a bad one. We need something better.

总结一下就是：
most bioinformatics software is not very good quality (#1), 
most bioinformatics software is not built by a team (#2), 
licensing is at best a minor component of what makes software widely used (#3), software should have an expiration date (#5), 
most URLs are unstable (#6), 
software should not be "idiot proof" (#7), 
and it shouldn't matter whether you use a specific programming language (#8).

有个博客对这篇文章进行了全面的评价：
http://ivory.idyll.org/blog//2015-response-to-software-myths.html
尤其是把Lior的第四点批评的体无完肤
What surprises me most about Lior's post, though, is that he's describing the present situation rather accurately, but he's not angry about it. I'm angry, frustrated, and upset by it, and I really want to see a better future -- I'm in science, and biology, partly because I think it can have a real impact on society and health. Software is a key part of that.

1. Somebody will build on your code.
Nope. Ok, maybe not never but almost certainly not. There are many reasons for this. The primary reason in my view is that most bioinformatics software is of very poor quality (more on why this is the case in #2). Who wants to read junk software, let alone try to edit it or build on it? Most bioinformatics software is also targeted at specific applications. Biologists who use application specific software are typically not interested in developing or improving software because methods development is not their main interest and software development is not their best skill. In the computational biology areas I have worked in during the past 20 years (and I have reviewed/tested/examined/used hundreds or even thousands of programs) I can count the software programs that have been extended or developed by someone other than the author on a single hand. Software that has been built on/extended is typically of the framework kind (e.g. SAMtools being a notable example) but even then development of code by anyone other than the author is rare. For example, for the FSA alignment project we used HMMoC, a convenient compiler for HMMs, but has anyone ever built on the codebase? Doesn’t look like it. You may have been told by your PI that your software will take on a life of its own, like Linux, but the data suggests that is simply not going to happen. No, Gnu is Not Unix and your spliced aligner is not the Linux kernel. Most likely you alone will be the only user of your software, so at least put in some comments, because otherwise the first time you have to fix your own bug you won’t remember what you were doing in the code, and that is just sad.
2. You should have assembled a team to build your software.
Nope. Although most corporate software these days is built by large teams working collaboratively, scientific software is different. I agree with James Taylor, who in the anatomy of successful computational biology software paper stated that ” A lot of traditional software engineering is about how to build software effectively with large teams, whereas the way most scientific software is developed is (and should be) different. Scientific software is often developed by one or a handful of people.” In fact, I knew you were a graduate student because most bioinformatics software is written singlehandedly by graduate students (occasionally by postdocs). This is actually problem (although not your fault!) Students such as yourself graduate, move on to other projects and labs, and stop maintaining (see #5), let alone developing their code. Many PIs insist on “owning” software their students wrote, hoping that new graduate students in their lab will develop projects of graduated students. But new students are reluctant to work on projects of others because in academia improvement of existing work garners much less credit than new work. After all, isn’t that why you were writing new software in the first place? I absolve you of your solitude, and encourage you to think about how you will create the right incentive structure for yourself to improve your software over a period of time that transcends your Ph.D. degree.
3. If you choose the right license more people will use and build on your program.
Nope. People have tried all sorts of licenses but the evidence suggests the success of software (as measured by usage, or development of the software by the computational biology community) is not correlated with any particular license. One of the most widely used software suites in bioinformatics (if not the most widely used) is the UCSC genome browser and its associated tools. The software is not free, in that even though it is free for academic, non-profit and personal use, it is sold commercially. It would be difficult to argue that this has impacted its use, or retarded its development outside of UCSC. To the contrary, it is almost inconceivable that anyone working in genetics, genomics or bioinformatics has notused the UCSC browser (on a regular basis). In fact, I have, during my entire career, heard of only one single person who claims not to use the browser; this person is apparently boycotting it due to the license. As far as development of the software, it has almost certainly been hacked/modified/developed by many academics and companies since its initial release (e.g. even within my own group). In anatomy of successful computational biology software published in Nature Biotechnology two years ago, a list of “software for the ages” consists of programs that utilize a wide variety of licenses, including Boost, BSD, and GPL/LGPL. If there is any pattern it is that the most common are GPL/LGPL, although I suspect that if one looks at bioinformatics software as a whole those licenses are also the most common in failed software. The key to successful software, it appears, is for it to be useful and usable. Worry more about that and less about the license, because ultimately helping biologists and addressing problems in biomedicine might be more ethical than hoisting the “right” software license flag.
4. Making your software free for commercial use shows you are not against companies.
Nope. The opposite is true. If you make your software free for commercial use, you are effectively creating a subsidy for companies, one that is funded by your university / your grants. You are a corporate hero! Congratulations! You have found a loophole for transferring scarce public money to the private sector. If you’ve licensed your software with BSD you’ve added another subsidy: a company using your software doesn’t have any reason to share their work with the academic community. There are two reasons why you might want to reconsider offering such subsidies. First, by denying yourself potential profits from sale of your software to industry, you are definitively removing any incentive for future development/maintenance of the software by yourself or future graduate students. Most bioinformatics software, when sold commercially, costs a few thousand dollars. This is a rounding error for companies developing cancer or other drugs at the cost of a billion dollars per drug and a tractable proposition even for startups, yet the money will make a real difference to you three years out from your Ph.D. when you’re earning a postdoc salary. A voice from the future tells you that you’ll appreciate the money, and it will help you remember that you really ought to fix that bug reported on GitHub posted two months ago. You will be part of the global improvement of bioinformatics software. And there is another reason to sell your software to companies: help your university incentivize more and better development of bioinformatics software. At most universities a majority of the royalties from software sales go to the institution (at UC Berkeley, where I work, its 2/3). Most schools, especially public universities, are struggling these days and have been for some time. Help them out in return for their investment in you; you’ll help generate more bioinformatics hires, and increase appreciation for your field. In other words, although it is not always practical or necessary, when possible, please sell your software commercially.
5. You should maintain your software indefinitely.
Nope. Someday you will die. Before that you will get a job, or not. Plan for your software to have a limited shelf-life, and act accordingly.
6. Your “stable URL” can exist forever.
Nope. When I started out as a computational biologist in the late 1990s I worked on genome alignment. At the time I was excited about Dynamite: a flexible code generating language for dynamic programming methods used in sequence comparison. This was a framework for specifying bioinformatics related dynamic programming algorithms, such as the Needleman-Wunsch or Smith-Waterman algorithms. The authors wrote that “A stable URL for Dynamite documentation, help and information ishttp://www.sanger.ac.uk/~birney/dynamite/” Of course the URL is long gone, and by no fault of the authors. The website hosting model of the late 1990s is long extinct. To his credit, Ewan now hosts the Dynamite code on GitHub, following a welcome trend that is likely to extend the life of bioinformatics programs in the future. Will GitHub be around forever? We’ll see. But more importantly, software becomes extinct (or ought to) for reasons other than just 404 errors. For example, returning to sequence alignment, the ClustalW program of 1994 was surpassed in accuracy and speed by many other multiple alignment programs developed in the 2000s. Yet people kept using ClustalW anyway, perhaps because it felt like a “safe bet” with its many citations (eventually in 2011 Clustalw was updated to Clustal Omega). The lag in improving ClustalW resulted in a lot of poor alignments being utilized in genomics studies for a decade (no fault of the authors of ClustalW, but harmful nonetheless). I’ve started the habit ofretiring my programs, via announcement on my website and PubMed. Please do the same when the time comes.
7. You should make your software “idiot proof”.
Nope. Your users, hopefully biologists (and not other bioinformatics programmers benchmarking your program to show that they beat it) are not idiots. Listen to them. Back in 2004 Nicolas Bray and I published a webserver for the alignment program MAVID. Users were required to input FASTA files. But can you guess why we had to write a script called checkfasta? (hint: the most popular word processor was and is Microsoft Word). We could have stopped there and laughed at our users, but after much feedback we realized the real issue was that FASTA was not necessarily the appropriate input to begin with. Our users wanted to be able to directly input Genbank accessions for alignment, and eventually Nicolas Bray wrote the perl scripts to allow them to do that (the feature lives on here). The take home message for you is that you should deal with user requests wisely, and your time will be needed not only to fix bugs but to address use cases and requested features you never thought of in the first place. Be prepared to treat your users respectfully, and you and your software will benefit enormously.
8. You used the right programming language for the task.
Nope. First it was perl, now it’s python. First it was MATLAB, now it’s R. First it was  C, then C++.  First it was multithreading now it’s Spark. There is always something new coming, which is yet another reason that almost certainly nobody is going to build on your code. By the time someone gets around to having to do something you may have worked on, there will be better ways. Therefore, the main thing is that your software should be written in a way that makes it easy to find and fix bugs, fast, and efficient (in terms of memory usage). If you can do that in Fortran great. In fact, in some fields not very far from bioinformatics, people do exactly that. My advice: stay away from Fortran (but do adopt some of the best practice advice offered here).
9. You should have read Lior Pachter’s blog post about the myths of bioinformatics software before starting your project.
Nope. Lior Pachter was an author on the Vista paper describing a program for which the source code was available only “upon request”.
New Insights into Human De Novo Mutations
July 30, 2015 by Dan Koboldt Leave a Comment
De novo mutations — sequence variants that are present in a child but absent from both parents — are an important source of human genetic variation. I think it’s reasonable to say that most of the 3-4 million variants in any individual’s genome arose, once upon a time, as de novo mutations in his or her ancestors. In the past few years, whole-genome sequencing (WGS) studies performed in families (especially parent-child trios) have offered some revelations about de novo mutations and their role in human disease, notably that:
o	The de novo mutation rate for humans is ~1.2e-08 per generation which works out to around 38 mutations genome-wide per offspring
o	95% of the global mutation rate is explained by paternal age (each year adding 1-2 mutations)
o	As much as 2/3 of genetic diagnoses from clinical sequencing efforts are de novo mutations.
A recent study in Nature Genetics provides the largest survey of de novo mutations to date. Laurent Francioli et al identified de novo mutations in 250 Dutch families that were sequenced to ~13x coverage as part of the Genome of the Netherlands (GoNL) project. Their findings confirm much of the observations from previous smaller studies, and offer some new insights into the patterns of de novo mutations throughout the human genome.
Identification of de novo Mutations
To make any global observations about de novo mutations, one generally needs unbiased whole-genome sequencing data for an individual and both parents. Even with those in hand, accurate identification of de novo mutations is challenging because they’re so exquisitely rare. Since the sequencing coverage in this study is a little bit light (13x, whereas most studies shoot for ~30x), I had some initial concerns about whether or not the mutation calls might hold up under scrutiny.
Delving into the online methods, I learned that the samples underwent Illumina paired-end sequencing (2x91bp, insert size 500bp). Alignment and variant calling followed GATK best practices (v2), and the mutations were called with the trio-aware GATK PhaseByTransmission. Next, the authors used a machine learning classifier trained on 592 true positive and 1,630 false positive de novo calls that had been validated experimentally. The net result was 11,020 high-confidence mutations in the 269 children, with an estimated a 92.2% accuracy.
The numbers are about right: if 92.2% of the calls are real, that’s 10,160 true mutations, or ~37.7 mutations per child. That’s very close to the estimated ~38 per genome. In other words, without experimentally validating all 11,000 mutations (an expensive and laborious task), this is as good as it gets.
Parent-of-Origin and Replication Timing
 
Credit: Francioli et al, Nature 2015
The authors first examined whether the location of the observed mutations was correlated with any epigenetic variables. There was no significant correlation for most of the variables examined (chromatin accessibility, histone modifications, and recombination rate).  With a linear regression model, they noted a significant association between replication timing and paternal age: mutations in the offspring of younger fathers (<28 years old) were strongly enriched in late-replicating regions, whereas mutations in offspring of older fathers were not.
To dig deeper, the authors looked at 2,621 mutations that could be unambiguously assigned to maternal or paternal origin. The method for this isn’t documented in the online methods, but presumably they looked for instances in which a mutation was in the same read or read pair as a variant unique to one parent. Notably, 1,991 of those origin-inferred mutations (76%) came from the father. After controlling for the number imbalance, thereplication-timing-with-parent-age correlation was significant only for mutations of paternal origin.
This makes a certain kind of sense, since the stem cells in the paternal germ line undergo continuous cell division throughout a man’s life, whereas a woman is born with all of the eggs she’ll ever have.
The correlation between paternal age and replication timing is important from a reproductive health perspective, because late-replicating regions have lower gene density and expression levels than early ones. Since the mutations in offspring of younger fathers tend to occur in these regions, they’re less likely to have a functional impact. In support of this idea, on average, the offspring born to 40-year-old fathers had twice as many genic mutations as offspring born to 20-year-old fathers.
In other words, mutations in the offspring of older fathers are not only more numerous, but also more likely to have functional consequences.
Mutations in Functional Regions
Notably, the de novo mutation rates in this study were higher in exonic regions regardless of the paternal age. Overall, 1.22% of mutations were exonic, an enrichment of 28.7% over simulated models of random mutation distribution. Mutations were also enriched in DNase I hypersensitive sites (DHSs), which represent likely regulatory regions. The source of this “functional enrichment” likely has to do with sequence context: mutations often occur at CpG dinucleotides, which are themselves more prevalent in exons and DHSs.
Recent studies of somatic mutations in tumor cells revealed a fascinating phenomenon: a reduction in the mutation rate of highly transcribed regions, likely attributed to the fidelity conferred by transcription-coupled DNA repair mechanisms. In the current study of de novo mutations, however, the mutation rate in transcribed regions and DHSs did not appear to be reduced.
The implication here might be that transcription-coupled repair has less of an impact on de novomutations, though the authors note that their study was only powered to detect a substantial difference (>17%) in mutation rate. That’s understandable, because while the individuals examined here harbored ~40 mutations genome-wide, a tumor specimen might have tens of thousands of somatic mutations (i.e. much better power to detect subtle differences in mutation rate).
Clustered de novo Mutations
One of the most interesting observations in this study was a clustering effect of de novo mutations. If all things were random, given the size of the genome (3.2 billion base pairs) and the number of mutations per individual (~40), we expect them to be pretty far apart. As in, one every 80 million base pairs.
Instead, the authors observed 78 instances in which there were “clusters” of 2-3 mutations within a 20kb window in the same individual. The 161 mutations involved showed no significant differences from the non-clustered mutations with regard to recombination rate (p=0.52) or replication timing (p=0.059), though I should point out that the latter might be approaching an interesting p-value.
Interestingly, however, the clustered mutations exhibited an unusual mutational spectrum, with astrong enrichment for C->G transversions compared to non-clustered mutations (p=1.8e-13).
 
Francioli et al, Nature 2015
Based on the nucleotide context, the authors suggest that a new mutational mechanism may be at work involving cytosine deamination of single-stranded DNA (presumably during replication). I don’t have strong enough chemistry to understand the proposed mechanism, but agree that this unusual pattern merits some more investigation.
References
Francioli LC, Polak PP, Koren A, Menelaou A, Chun S, Renkens I, Genome of the Netherlands Consortium, van Duijn CM, Swertz M, Wijmenga C, van Ommen G, Slagboom PE, Boomsma DI, Ye K, Guryev V, Arndt PF, Kloosterman WP, de Bakker PI, & Sunyaev SR (2015). Genome-wide patterns and properties of de novo mutations in humans. Nature genetics, 47 (7), 822-6 PMID: 25985141

为什么要从事生物信息学研究呢？
Web: http://blog.thegrandlocus.com/2015/05/why-do-bioinformatics
我以前从未想过自己会从事生物信息学研究的，只是因为比较喜欢在电脑面前工作的感觉，所以就做了这个明智的决定。有时候我也想，我的生命中还有很多其它事情等着我去完成，也会怀疑，我现在的选择是最好的吗？正巧我最近看了一篇Frederick的文章《farewell to bioinformatics》，里面讨论了关于生物信息学作为职业的问题，很值得一读。里面最体现了作者观点一句话是：就我从事生物信息学研究的经验来看，我对它的态度很明确，总结起来就是： Fuck you, bioinformatics. Eat shit and die.

对于这样的观点，我不予置否。但是Frederick在他的博客里面对于他的见解做了深度阐述。总而言之，他认为生物信息学家不是一个好的程序员，大部分的工作都是在自欺欺人。总是创造一些没人要的软件，规定一大堆各种文件格式，也不公布算法，这样使得他人无法重复他的工作，这几点使得他们的工作产能减少90%，甚至可以说99%都是无用功。

生物信息学的障碍
Web：http://madhadron.com/posts/2012-03-26-a-farewell-to-bioinformatics.html
现在我已经离开了生物信息学研究领域，去了一家软件公司，那里有很多技术牛人，而且我也可以拿到更多的报酬。这似乎意味着终于有了个合适的机会来说说我对生物信息学的看法。

就我从事生物信息学研究的经验来看，我对它的态度很明确，总结起来就是： Fuck you, bioinformatics. Eat shit and die.
生物信息学的作用应该是力图使得微观世界的分子生物学更容易理解。所有的分子生物学家缺乏那些实验室外的技术



